{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 등장하는 단어의 빈도수를 계산해서 문장 분류하기\n",
    "\n",
    "[원본](https://blog.eduonix.com/internet-of-things/simple-nlp-based-chatbot-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    "\n",
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['Hi',\n",
    "     'Hello',\n",
    "     'How are you?',\n",
    "     'I am studying',\n",
    "     'studying',\n",
    "     'see you later',\n",
    "     'bye',\n",
    "     'goodbye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ['greeting',\n",
    "     'greeting',\n",
    "     'greeting',\n",
    "     'studying',\n",
    "     'studying',\n",
    "     'bye',\n",
    "     'bye',\n",
    "     'bye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자, 숫자를 제외한 문장을 반환하는 함수\n",
    "def remove_non_alpha_numeric_characters(sentence):\n",
    "    new_sentence = ''\n",
    "    for alphabet in sentence:\n",
    "        if alphabet.isalpha() or alphabet == ' ':\n",
    "            new_sentence += alphabet\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "# 소문자, 단어 앞뒤 공백제거 \n",
    "def preprocess_data(X):\n",
    "    X = [data_point.lower() for data_point in X]\n",
    "    X = [remove_non_alpha_numeric_characters(\n",
    "        sentence) for sentence in X]\n",
    "    X = [data_point.strip() for data_point in X]\n",
    "    X = [re.sub(' +', ' ',\n",
    "                data_point) for data_point in X]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어사전 생성\n",
    "X = preprocess_data(X)\n",
    "\n",
    "vocabulary = set()\n",
    "for data_point in X:\n",
    "    for word in data_point.split(' '):\n",
    "        vocabulary.add(word)\n",
    "\n",
    "vocabulary = list(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코딩\n",
    "X_encoded = []\n",
    "\n",
    "def encode_sentence(sentence):\n",
    "    sentence = preprocess_data([sentence])[0]\n",
    "    sentence_encoded = [0] * len(vocabulary)\n",
    "    for i in range(len(vocabulary)):\n",
    "        if vocabulary[i] in sentence.split(' '):\n",
    "            sentence_encoded[i] = 1\n",
    "    return sentence_encoded\n",
    "\n",
    "X_encoded = [encode_sentence(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent 인코팅\n",
    "classes = list(set(Y))\n",
    "\n",
    "Y_encoded = []\n",
    "for data_point in Y:\n",
    "    data_point_encoded = [0] * len(classes)\n",
    "    for i in range(len(classes)):\n",
    "        if classes[i] == data_point:\n",
    "            data_point_encoded[i] = 1\n",
    "    Y_encoded.append(data_point_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할, 학습 데이터, 테스트 데이터\n",
    "X_train = np.array(X_encoded)\n",
    "y_train = np.array(Y_encoded)\n",
    "X_test = np.array(X_encoded)\n",
    "y_test = np.array(Y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.6075\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 371us/step - loss: 1.5417\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.4583\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 625us/step - loss: 1.3679\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 494us/step - loss: 1.2800\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 495us/step - loss: 1.2031\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.1438\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 619us/step - loss: 1.1064\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 1.0912\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0938\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 619us/step - loss: 1.1067\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 372us/step - loss: 1.1219\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 749us/step - loss: 1.1332\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 622us/step - loss: 1.1374\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 500us/step - loss: 1.1337\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 371us/step - loss: 1.1237\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 626us/step - loss: 1.1100\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 496us/step - loss: 1.0951\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 1.0813\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0699\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 495us/step - loss: 1.0614\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 495us/step - loss: 1.0555\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 1.0516\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0490\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0470\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 1.0450\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 1.0426\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0398\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0364\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 1.0325\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 1.0284\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0242\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 1.0201\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0162\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 1.0124\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 1.0089\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 1.0055\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 249us/step - loss: 1.0023\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 249us/step - loss: 0.9991\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 748us/step - loss: 0.9959\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9927\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 0.9895\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9862\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 498us/step - loss: 0.9829\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9796\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9764\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9731\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9698\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9665\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9632\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9600\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 748us/step - loss: 0.9567\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9535\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9502\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9470\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9437\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9405\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9373\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9340\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9308\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9275\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9243\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9210\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9178\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9145\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9113\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9080\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.9047\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.9015\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8982\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8949\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8916\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8884\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8851\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8818\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8785\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8752\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8719\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8686\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8653\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8620\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8587\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8554\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 0.8521\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8487\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8454\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 249us/step - loss: 0.8421\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8387\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 0.8354\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 0.8320\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8287\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8253\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8220\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.8186\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8153\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8119\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8085\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8052\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.8018\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 374us/step - loss: 0.7984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20803f4cc88>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='sigmoid',\n",
    "                input_dim=len(X_train[0])))\n",
    "model.add(Dense(units=len(y_train[0]), activation='softmax'))\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.01,\n",
    "                            momentum=0.9, nesterov=True))\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [argmax(pred) for pred in model.predict(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 8\n",
      "Total: 8\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == argmax(y_test[i]):\n",
    "        correct += 1\n",
    "\n",
    "print(\"Correct:\", correct)\n",
    "print(\"Total:\", len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence\n",
      "hi\n",
      "greeting\n",
      "Enter a sentence\n",
      "hello\n",
      "greeting\n",
      "Enter a sentence\n",
      "demo\n",
      "bye\n",
      "Enter a sentence\n",
      "i am study\n",
      "greeting\n",
      "Enter a sentence\n",
      "i am studying\n",
      "studying\n",
      "Enter a sentence\n",
      "bye\n",
      "bye\n",
      "Enter a sentence\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"Enter a sentence\")\n",
    "    # python 2.7\n",
    "    # sentence = raw_input() \n",
    "    sentence = input()\n",
    "    prediction= model.predict(np.array([encode_sentence(sentence)]))\n",
    "    print(classes[argmax(prediction)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
